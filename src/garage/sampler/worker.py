"""Worker class used in all Samplers."""
import abc
from collections import defaultdict

import numpy as np

from garage.experiment import deterministic


class Worker(abc.ABC):
    """Worker class used in all Samplers."""

    def __init__(self, seed, max_path_length, worker_number):
        """Initialize a worker.

        Args:
            seed(int): The seed to use to intialize random number generators.
            max_path_length(int): The maximum length paths which will be
                sampled.
            worker_number(int): The number of the worker this update is
                occurring in. This argument is used to set a different seed for
                each worker.

        Should create fields the following fields:
            agent(Policy or None): The worker's initial agent.
            env(gym.Env or None): The worker's environment.
        """

    def update_agent(self, agent_update):
        """Update the worker's agent, using agent_update."""

    def update_env(self, env_update):
        """Update the worker's env, using env_update."""

    def rollout(self):
        """Sample a single rollout of the agent in the environment.

        Returns:
            observations(np.array): Non-flattened array of observations.
            actions(np.array): Non-flattened array of actions.
            rewards(np.array): Array of rewards of shape (timesteps, 1).
            agent_infos(Dict[str, np.array]): Dictionary of stacked,
                non-flattened `agent_info`s.
            env_infos(Dict[str, np.array]): Dictionary of stacked,
                non-flattened `env_info`s.

        """


class DefaultWorker(Worker):
    """Initialize a worker.

    Args:
        seed(int): The seed to use to intialize random number generators.
        max_path_length(int): The maximum length paths which will be sampled.
        worker_number(int): The number of the worker this update is
            occurring in. This argument is used to set a different seed for
            each worker.

    Should create fields the following fields:
        agent(Policy or None): The worker's initial agent.
        env(gym.Env or None): The worker's environment.
    """

    def __init__(self, seed, max_path_length, worker_number):
        self.seed = seed
        self.max_path_length = max_path_length
        self.worker_number = worker_number
        self.agent = None
        self.env = None
        self.worker_init()

    def worker_init(self):
        """Initialize a worker.

        Args:
            worker_number(int): The number of the worker this update is
                occurring in. This argument is used to set a different seed for
                each worker.
        """
        deterministic.set_seed(self.seed + self.worker_number)

    def update_agent(self, agent_update):
        """Update an agent, assuming it implements garage.tf.policies.Policy2.

        Args:
            agent_update(Dict[str, np.array]): Parameters to agent, which
                should have been generated by calling
                `policy.get_param_values`.

        """
        self.agent.set_param_values(agent_update)

    def update_env(self, env_update):
        """Use any non-None env_update as a new environment.

        A simple env update function. If env_update is not None, it should be
        the complete new environment.

        This allows changing environments by passing the new environment as
        `env_update` into `obtain_samples`.

        Args:
            env_update(gym.Env or None): The environment to replace the
                existing env with.

        Returns:
            The updated environment.

        """
        if env_update is not None:
            self.env = env_update

    def rollout(self):
        """Sample a single rollout of the agent in the environment.

        Returns:
            observations(np.array): Non-flattened array of observations.
            actions(np.array): Non-flattened array of actions.
            rewards(np.array): Array of rewards of shape (timesteps, 1).
            agent_infos(Dict[str, np.array]): Dictionary of stacked,
                non-flattened `agent_info`s.
            env_infos(Dict[str, np.array]): Dictionary of stacked,
                non-flattened `env_info`s.

        """
        observations = []
        actions = []
        rewards = []
        agent_infos = defaultdict(list)
        env_infos = defaultdict(list)
        o = self.env.reset()
        self.agent.reset()
        next_o = None
        path_length = 0
        while path_length < self.max_path_length:
            a, agent_info = self.agent.get_action(o)
            next_o, r, d, env_info = self.env.step(a)
            observations.append(o)
            rewards.append(r)
            actions.append(a)
            for k, v in agent_info.items():
                agent_infos[k].append(v)
            for k, v in env_info.items():
                env_infos[k].append(v)
            path_length += 1
            if d:
                break
            o = next_o
        for k, v in agent_infos.items():
            agent_infos[k] = np.asarray(v)
        for k, v in env_infos.items():
            env_infos[k] = np.asarray(v)
        return (np.array(observations), np.array(actions), np.array(rewards),
                dict(agent_infos), dict(env_infos))
